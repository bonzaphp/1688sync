---
name: "调度队列"
status: "open"
created: "2025-11-18T13:09:28Z"
updated: 
github: https://github.com/bonzaphp/1688sync/issues/4
depends_on: ["001"]
parallel: true
conflicts_with: ""
---

# 任务: 调度队列

## Overview

构建1688sync项目的任务调度队列系统，实现高效的异步任务管理和调度控制。系统需要支持大规模商品数据的并发处理，提供任务优先级管理、失败重试机制、实时状态监控等功能，确保整个爬虫系统的高效稳定运行。

## Acceptance Criteria

### 核心功能要求
- [ ] **任务队列架构设计**
  - 实现基于Redis的分布式任务队列
  - 设计任务数据结构和状态管理
  - 支持多种任务类型（单商品抓取、批量同步等）
  - 实现任务生命周期管理

- [ ] **任务调度器**
  - 实现FIFO、LIFO、优先级调度算法
  - 支持定时任务和周期任务
  - 实现任务分发和负载均衡
  - 支持任务抢占和取消机制

- [ ] **并发控制系统**
  - 实现工作进程池管理
  - 支持动态并发数调整
  - 实现资源配额和使用限制
  - 支持优雅关闭和重启机制

- [ ] **失败重试机制**
  - 实现指数退避重试策略
  - 配置最大重试次数和间隔
  - 支持失败任务分类处理
  - 实现死信队列管理

- [ ] **实时状态监控**
  - 提供任务执行状态查询
  - 实现性能指标统计
  - 支持实时日志和告警
  - 提供Web监控界面

### 技术实现要求
- [ ] **任务队列核心**
  - 基于Celery或自研队列系统
  - 支持Redis作为消息broker
  - 实现任务持久化和恢复
  - 支持任务分组和批处理

- [ ] **任务管理API**
  - 提供RESTful API接口
  - 支持任务创建、查询、取消操作
  - 实现任务参数验证和错误处理
  - 提供批量操作接口

- [ ] **调度策略实现**
  - 实现优先级队列算法
  - 支持定时任务调度
  - 实现任务依赖关系管理
  - 支持任务流编排

- [ ] **监控告警系统**
  - 实现任务执行统计
  - 支持性能指标收集
  - 提供实时状态看板
  - 实现异常告警机制

- [ ] **运维工具**
  - 提供队列管理命令行工具
  - 实现任务清理和修复工具
  - 支持队列健康检查
  - 提供性能调优建议

## Technical Details

### 系统架构设计
```
调度队列系统架构:
┌─────────────────┐
│   API接口层     │ ← RESTful API + Web UI
├─────────────────┤
│   调度控制器    │ ← 任务调度、优先级管理
├─────────────────┤
│   任务队列层    │ ← Redis + 消息分发
├─────────────────┤
│   执行工作层    │ ← Worker进程池
├─────────────────┤
│   状态监控层    │ ← 统计、监控、告警
└─────────────────┘
```

### 任务队列核心设计

#### 1. 任务定义
```python
from dataclasses import dataclass
from enum import Enum
from typing import Dict, Any, Optional
import uuid

class TaskType(Enum):
    """任务类型枚举"""
    SINGLE_PRODUCT = "single_product"
    BATCH_PRODUCTS = "batch_products"
    SEARCH_KEYWORDS = "search_keywords"
    SHOP_CRAWL = "shop_crawl"
    FULL_SYNC = "full_sync"

class TaskStatus(Enum):
    """任务状态枚举"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"
    RETRYING = "retrying"

@dataclass
class Task:
    """任务数据模型"""
    id: str
    type: TaskType
    status: TaskStatus
    priority: int = 1  # 1-10，10最高
    params: Dict[str, Any]
    created_at: float
    updated_at: float
    started_at: Optional[float] = None
    finished_at: Optional[float] = None
    retry_count: int = 0
    max_retries: int = 3
    error_message: Optional[str] = None
    result: Optional[Dict] = None
    depends_on: list = None  # 依赖的任务ID列表

    def __post_init__(self):
        if self.id is None:
            self.id = str(uuid.uuid4())
        if self.depends_on is None:
            self.depends_on = []
```

#### 2. 任务队列实现
```python
import asyncio
import json
import time
from typing import List, Callable, Optional
from dataclasses import asdict
import redis.asyncio as redis

class TaskQueue:
    """基于Redis的分布式任务队列"""

    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)
        self.queues = {
            'high': 10,      # 高优先级
            'normal': 5,     # 普通优先级
            'low': 1,        # 低优先级
            'retry': 0       # 重试队列
        }
        self.running = False
        self.workers = []
        self.task_handlers: Dict[TaskType, Callable] = {}

    async def submit_task(self, task: Task) -> str:
        """提交任务到队列"""
        # 验证任务参数
        self._validate_task(task)

        # 序列化任务
        task_data = json.dumps(asdict(task), ensure_ascii=False)

        # 根据优先级选择队列
        queue_name = self._get_queue_name(task.priority)

        # 添加到相应队列
        await self.redis.lpush(queue_name, task_data)

        # 更新任务状态
        task.status = TaskStatus.PENDING
        await self._save_task(task)

        return task.id

    async def start_workers(self, worker_count: int = 5):
        """启动工作进程"""
        self.running = True
        for i in range(worker_count):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)

    async def _worker(self, worker_id: str):
        """工作进程主循环"""
        while self.running:
            try:
                # 从高优先级队列开始检查
                task_data = None
                queue_name = None

                for priority_queue in ['retry', 'high', 'normal', 'low']:
                    task_data = await self.redis.brpop(priority_queue, timeout=1)
                    if task_data:
                        queue_name = priority_queue
                        task_data = task_data[1].decode()
                        break

                if not task_data:
                    continue

                # 反序列化任务
                task_dict = json.loads(task_data)
                task = Task(**task_dict)
                task.status = TaskStatus.RUNNING
                task.started_at = time.time()

                await self._save_task(task)

                # 执行任务
                try:
                    result = await self._execute_task(task)
                    task.status = TaskStatus.SUCCESS
                    task.result = result
                except Exception as e:
                    task.error_message = str(e)
                    await self._handle_task_failure(task)

                task.finished_at = time.time()
                task.updated_at = time.time()
                await self._save_task(task)

            except Exception as e:
                print(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)

    async def _execute_task(self, task: Task) -> Dict:
        """执行具体任务"""
        handler = self.task_handlers.get(task.type)
        if not handler:
            raise ValueError(f"No handler for task type: {task.type}")

        return await handler(task)

    async def _handle_task_failure(self, task: Task):
        """处理任务失败"""
        if task.retry_count < task.max_retries:
            # 指数退避策略
            task.retry_count += 1
            delay = 2 ** task.retry_count
            task.status = TaskStatus.RETRYING

            # 延迟后重新加入队列
            await asyncio.sleep(delay)
            await self.submit_task(task)
        else:
            task.status = TaskStatus.FAILED

    def register_handler(self, task_type: TaskType, handler: Callable):
        """注册任务处理器"""
        self.task_handlers[task_type] = handler
```

#### 3. 任务调度器
```python
class TaskScheduler:
    """任务调度器"""

    def __init__(self, queue: TaskQueue):
        self.queue = queue
        self.scheduled_tasks = {}  # 定时任务存储
        self.running = False

    async def schedule_repeated_task(self, task: Task, interval: int):
        """调度重复任务"""
        self.scheduled_tasks[task.id] = {
            'task': task,
            'interval': interval,
            'last_run': 0
        }

        if not self.running:
            await self.start()

    async def start(self):
        """启动调度器"""
        self.running = True
        asyncio.create_task(self._scheduler_loop())

    async def _scheduler_loop(self):
        """调度器主循环"""
        while self.running:
            current_time = time.time()

            for task_id, task_info in self.scheduled_tasks.items():
                last_run = task_info['last_run']
                interval = task_info['interval']

                if current_time - last_run >= interval:
                    # 创建新任务实例
                    new_task = Task(
                        id=str(uuid.uuid4()),
                        type=task_info['task'].type,
                        status=TaskStatus.PENDING,
                        priority=task_info['task'].priority,
                        params=task_info['task'].params.copy(),
                        created_at=current_time,
                        updated_at=current_time
                    )

                    await self.queue.submit_task(new_task)
                    task_info['last_run'] = current_time

            await asyncio.sleep(10)  # 每10秒检查一次

    async def cancel_scheduled_task(self, task_id: str):
        """取消定时任务"""
        if task_id in self.scheduled_tasks:
            del self.scheduled_tasks[task_id]
```

### 任务处理器实现

#### 1. 单商品抓取处理器
```python
async def handle_single_product_crawl(task: Task) -> Dict:
    """单商品抓取处理器"""
    product_url = task.params.get('url')
    if not product_url:
        raise ValueError("Product URL is required")

    # 调用爬虫抓取商品数据
    crawler_service = CrawlerService()
    product_data = await crawler_service.crawl_product(product_url)

    # 存储到数据库
    storage_service = StorageService()
    await storage_service.save_product(product_data)

    return {
        'product_id': product_data['id'],
        'url': product_url,
        'status': 'success'
    }

# 注册处理器
queue.register_handler(TaskType.SINGLE_PRODUCT, handle_single_product_crawl)
```

#### 2. 批量抓取处理器
```python
async def handle_batch_products_crawl(task: Task) -> Dict:
    """批量商品抓取处理器"""
    urls = task.params.get('urls', [])
    batch_size = task.params.get('batch_size', 10)

    results = {
        'total': len(urls),
        'success': 0,
        'failed': 0,
        'details': []
    }

    # 分批处理
    for i in range(0, len(urls), batch_size):
        batch_urls = urls[i:i + batch_size]
        batch_tasks = []

        for url in batch_urls:
            # 创建子任务
            sub_task = Task(
                id=str(uuid.uuid4()),
                type=TaskType.SINGLE_PRODUCT,
                status=TaskStatus.PENDING,
                priority=task.priority - 1,  # 子任务优先级稍低
                params={'url': url},
                created_at=time.time(),
                updated_at=time.time()
            )
            batch_tasks.append(sub_task)

        # 并发执行批次任务
        tasks = [queue.execute_task(sub_task) for sub_task in batch_tasks]
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)

        # 统计结果
        for result in batch_results:
            if isinstance(result, Exception):
                results['failed'] += 1
                results['details'].append({'error': str(result)})
            else:
                results['success'] += 1
                results['details'].append(result)

    return results
```

### 监控和统计

#### 1. 任务统计
```python
class TaskStats:
    """任务统计类"""

    def __init__(self, redis_client):
        self.redis = redis_client

    async def record_task_start(self, task_id: str, task_type: str):
        """记录任务开始"""
        await self.redis.set(f"task:{task_id}:start", time.time())
        await self.redis.incr(f"stats:tasks:{task_type}:started")

    async def record_task_finish(self, task_id: str, status: TaskStatus):
        """记录任务完成"""
        start_time = await self.redis.get(f"task:{task_id}:start")
        if start_time:
            duration = time.time() - float(start_time)
            await self.redis.set(f"task:{task_id}:duration", duration)

        await self.redis.incr(f"stats:tasks:{status.value}:completed")

    async def get_queue_stats(self) -> Dict:
        """获取队列统计信息"""
        stats = {}
        for priority in ['high', 'normal', 'low', 'retry']:
            stats[f"{priority}_queue_size"] = await self.redis.llen(priority)

        return stats
```

#### 2. 监控API
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="1688sync Queue Monitor")

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    """查询任务状态"""
    task = await queue.get_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    return asdict(task)

@app.post("/api/tasks")
async def submit_task(task_data: dict):
    """提交新任务"""
    task = Task(
        id=str(uuid.uuid4()),
        type=TaskType(task_data['type']),
        status=TaskStatus.PENDING,
        priority=task_data.get('priority', 5),
        params=task_data['params'],
        created_at=time.time(),
        updated_at=time.time()
    )

    task_id = await queue.submit_task(task)
    return {"task_id": task_id, "status": "submitted"}

@app.get("/api/stats")
async def get_queue_stats():
    """获取队列统计"""
    return await stats.get_queue_stats()

@app.get("/api/stats/tasks")
async def get_task_stats():
    """获取任务统计"""
    return await stats.get_task_stats()
```

### 配置管理

#### 队列配置
```python
# config/queue.yaml
redis:
  url: "redis://localhost:6379/0"
  max_connections: 20
  connection_pool_timeout: 5

queue:
  priorities:
    high: 10
    normal: 5
    low: 1
  retry:
    max_retries: 3
    base_delay: 1
    max_delay: 60

workers:
  default_count: 5
  max_count: 20
  task_timeout: 300
  shutdown_timeout: 30

monitoring:
  stats_interval: 60
  health_check_interval: 30
  alert_thresholds:
    failure_rate: 0.1
    queue_size: 1000
    avg_duration: 300
```

### 运维工具

#### 命令行管理工具
```python
import click
import asyncio

@click.group()
def cli():
    """1688sync 队列管理工具"""
    pass

@cli.command()
@click.option('--count', default=5, help='Worker进程数量')
def start_workers(count: int):
    """启动工作进程"""
    async def _start():
        queue = TaskQueue("redis://localhost:6379/0")
        await queue.start_workers(count)
        print(f"Started {count} workers")
        await asyncio.Event().wait()  # 永远等待

    asyncio.run(_start())

@cli.command()
@click.option('--task-type', required=True, help='任务类型')
@click.option('--params', required=True, help='任务参数(JSON格式)')
@click.option('--priority', default=5, help='优先级(1-10)')
def submit_task(task_type: str, params: dict, priority: int):
    """提交新任务"""
    async def _submit():
        queue = TaskQueue("redis://localhost:6379/0")
        task = Task(
            id=str(uuid.uuid4()),
            type=TaskType(task_type),
            status=TaskStatus.PENDING,
            priority=priority,
            params=json.loads(params),
            created_at=time.time(),
            updated_at=time.time()
        )
        task_id = await queue.submit_task(task)
        print(f"Task submitted: {task_id}")

    asyncio.run(_submit())

@cli.command()
def stats():
    """显示队列统计信息"""
    async def _stats():
        queue = TaskQueue("redis://localhost:6379/0")
        stats = await queue.get_queue_stats()
        for key, value in stats.items():
            print(f"{key}: {value}")

    asyncio.run(_stats())
```

## Effort Estimate

### 工作量评估
- **开发时间**: 18-22个工作日
- **任务分解**:
  - 任务队列核心实现: 6天
  - 任务调度器开发: 4天
  - 任务处理器实现: 4天
  - 监控统计系统: 3天
  - Web监控界面: 3天
  - 测试和优化: 2-3天

### 技能要求
- **必备技能**:
  - Redis使用经验
  - 异步编程和并发控制
  - 分布式系统设计
  - Celery或类似队列系统经验

- **推荐技能**:
  - FastAPI框架使用
  - WebSocket实时通信
  - 系统监控和告警
  - 容器化部署经验

### 风险评估
- **高风险**: Redis故障导致任务丢失
- **中风险**: 大量并发任务下的性能瓶颈
- **低风险**: 任务状态不一致问题

## Dependencies

### 内部依赖
- **任务001**: 存储系统 - 提供数据持久化服务
- **任务002**: 核心技术 - 提供爬虫服务接口

### 外部依赖
- Redis数据库
- FastAPI框架（Web监控界面）

## Quality Gates

- [ ] **功能测试**: 成功处理1000个并发任务，成功率≥95%
- [ ] **性能测试**: 单任务平均处理时间≤3秒，队列响应时间≤100ms
- [ ] **稳定性测试**: 连续运行72小时，内存使用稳定
- [ ] **监控完整性**: 实时监控指标准确，告警及时有效

## Deliverables

1. **任务队列系统完整代码**
2. **Web监控界面和API文档**
3. **任务调度配置文件**
4. **运维管理命令行工具**
5. **性能测试报告和部署指南**
