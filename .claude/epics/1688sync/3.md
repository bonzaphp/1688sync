---
name: "核心技术"
status: "closed"
created: "2025-11-18T13:09:28Z"
updated: "2025-11-21T14:30:00Z"
github: ""
depends_on: ["001"]
parallel: true
conflicts_with: ""
---

# 任务: 核心技术

## Overview

基于Scrapy框架构建1688商品数据爬虫系统，作为1688sync项目的核心数据获取引擎。系统需要实现高效、稳定的商品数据抓取能力，支持多种抓取模式，处理反爬虫机制，并确保数据质量和完整性。系统将提供丰富的配置选项，支持大规模商品数据的并发抓取。

## Acceptance Criteria

### 核心功能要求
- [ ] **Scrapy项目架构设计**
  - 建立完整的Scrapy项目结构
  - 设计可复用的Spider基类和工具类
  - 实现配置驱动的爬虫系统
  - 支持多种抓取场景（单商品、列表页、搜索）

- [ ] **商品数据抓取器**
  - 实现商品详情页数据抓取Spider
  - 抓取字段：标题、价格、规格、图片、销量等
  - 支持动态内容加载（JavaScript渲染）
  - 处理商品不同类型的数据结构差异

- [ ] **反爬虫应对策略**
  - 实现User-Agent轮换机制
  - 配置代理IP池支持
  - 实现请求频率控制和延迟策略
  - 处理验证码和人机验证
  - Cookie管理和会话保持

- [ ] **并发控制机制**
  - 配置Scrapy并发请求参数
  - 实现下载延迟和超时控制
  - 支持断点续传功能
  - 实现任务队列和优先级管理

- [ ] **数据提取和清洗**
  - 使用XPath/CSS选择器提取数据
  - 实现数据清洗和格式化逻辑
  - 处理数据缺失和异常情况
  - 确保数据字段完整性和准确性

### 技术实现要求
- [ ] **多模式抓取支持**
  - 单商品链接抓取模式
  - 商品列表页批量抓取
  - 关键词搜索结果抓取
  - 店铺内商品全量抓取

- [ ] **数据管道处理**
  - 实现Scrapy Item Pipeline
  - 数据验证和类型检查
  - 数据转换和格式化
  - 错误数据处理和重试机制

- [ ] **中间件系统**
  - 实现下载中间件（代理、User-Agent等）
  - 实现爬虫中间件（请求预处理、响应后处理）
  - 错误处理和重试中间件
  - 性能监控和统计中间件

- [ ] **配置管理系统**
  - 统一的配置文件管理
  - 环境特定配置（开发/测试/生产）
  - 运行时配置动态更新
  - 配置验证和错误提示

- [ ] **监控和日志系统**
  - 详细的爬虫执行日志
  - 实时统计信息（成功率、速度等）
  - 错误日志和异常告警
  - 性能指标收集和展示

## Technical Details

### 系统架构设计
```
爬虫系统架构:
┌─────────────────┐
│   调度控制层    │ ← 任务调度、并发控制
├─────────────────┤
│   Spider执行层  │ ← 多种Spider实现
├─────────────────┤
│   数据处理层    │ ← 提取、清洗、验证
├─────────────────┤
│   中间件层      │ ← 下载、爬虫中间件
├─────────────────┤
│   网络请求层    │ ← HTTP客户端、代理
└─────────────────┘
```

### Spider设计

#### 1. 商品详情页Spider
```python
import scrapy
from itemloaders import ProductItemLoader

class ProductDetailSpider(scrapy.Spider):
    name = "product_detail"

    def __init__(self, product_urls=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start_urls = product_urls.split(',') if product_urls else []

    def parse(self, response):
        """解析商品详情页"""
        loader = ProductItemLoader(response=response)

        # 基本信息提取
        loader.add_xpath('title', '//h1[@class="product-title"]/text()')
        loader.add_xpath('price', '//span[@class="price-current"]/text()')
        loader.add_xpath('moq', '//div[@class="moq-info"]/text()')

        # 图片提取
        loader.add_xpath('main_image', '//img[@id="product-image"]/@src')
        loader.add_xpath('detail_images', '//div[@class="image-gallery"]//img/@src')

        # 规格参数
        loader.add_xpath('specifications', '//table[@class="spec-table"]')

        # 销量信息
        loader.add_xpath('sales_count', '//span[@class="sales-count"]/text()')
        loader.add_xpath('supplier_info', '//div[@class="supplier-info"]')

        yield loader.load_item()
```

#### 2. 搜索结果Spider
```python
class ProductSearchSpider(scrapy.Spider):
    name = "product_search"

    def __init__(self, keyword=None, max_pages=10, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.keyword = keyword
        self.max_pages = max_pages

    def start_requests(self):
        """生成搜索请求"""
        for page in range(1, self.max_pages + 1):
            url = f"https://search.1688.com/trade/search?keywords={self.keyword}&page={page}"
            yield scrapy.Request(
                url=url,
                callback=self.parse_search_results,
                meta={'page': page}
            )

    def parse_search_results(self, response):
        """解析搜索结果页"""
        # 提取商品链接
        product_links = response.css('a.product-link::attr(href)').getall()

        for link in product_links:
            # 访问商品详情页
            yield response.follow(
                url=link,
                callback=self.parse_product_detail,
                meta={'source': 'search', 'keyword': self.keyword}
            )

        # 处理翻页
        current_page = response.meta['page']
        if current_page < self.max_pages:
            next_page = current_page + 1
            next_url = f"https://search.1688.com/trade/search?keywords={self.keyword}&page={next_page}"
            yield response.follow(
                url=next_url,
                callback=self.parse_search_results,
                meta={'page': next_page}
            )
```

#### 3. 列表页Spider
```python
class ProductListSpider(scrapy.Spider):
    name = "product_list"

    def parse_list_page(self, response):
        """解析商品列表页"""
        # 提取商品详情页链接
        product_items = response.css('div.product-item')

        for item in product_items:
            product_url = item.css('a::attr(href)').get()
            if product_url:
                yield response.follow(
                    url=product_url,
                    callback=self.parse_product_detail,
                    meta={'category': response.meta.get('category')}
                )

        # 处理翻页
        next_page = response.css('a.next-page::attr(href)').get()
        if next_page:
            yield response.follow(
                url=next_page,
                callback=self.parse_list_page,
                meta={'category': response.meta.get('category')}
            )
```

### 数据提取策略

#### XPath选择器优化
```python
# 高效的XPath选择器
SELECTORS = {
    'product': {
        'title': '//h1[contains(@class, "product-title")]/text()',
        'price_range': '//span[contains(@class, "price")]//text()',
        'moq': '//div[contains(@class, "moq")]//text()',
        'main_image': '//img[@id="main-image"]/@src',
        'detail_images': '//div[contains(@class, "gallery")]//img/@src',
        'specifications': '//table[contains(@class, "spec")]//tr',
        'supplier': {
            'name': '//div[contains(@class, "supplier-name")]/text()',
            'rating': '//span[contains(@class, "rating")]/text()',
            'location': '//div[contains(@class, "location")]/text()',
        }
    }
}
```

#### 数据清洗逻辑
```python
def clean_price(price_text):
    """清洗价格数据"""
    if not price_text:
        return None, None

    import re
    # 提取价格数字
    price_pattern = r'[\d,]+\.?\d*'
    prices = re.findall(price_pattern, price_text.replace(',', ''))

    if len(prices) >= 2:
        return float(prices[0]), float(prices[1])  # min, max
    elif len(prices) == 1:
        return float(prices[0]), float(prices[0])
    else:
        return None, None

def clean_image_urls(image_urls):
    """清洗图片URL"""
    cleaned_urls = []
    for url in image_urls:
        if url.startswith('//'):
            url = 'https:' + url
        elif not url.startswith('http'):
            url = 'https://' + url
        cleaned_urls.append(url)
    return cleaned_urls
```

### 反爬虫应对策略

#### User-Agent轮换
```python
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
    # 更多User-Agent...
]

class RotateUserAgentMiddleware:
    def process_request(self, request, spider):
        ua = random.choice(USER_AGENTS)
        request.headers['User-Agent'] = ua
```

#### 代理IP池管理
```python
class ProxyMiddleware:
    def __init__(self):
        self.proxies = self.load_proxies()

    def load_proxies(self):
        """从配置文件加载代理列表"""
        with open('config/proxies.json', 'r') as f:
            return json.load(f)

    def process_request(self, request, spider):
        if self.proxies:
            proxy = random.choice(self.proxies)
            request.meta['proxy'] = proxy
```

#### 请求频率控制
```python
class ThrottleMiddleware:
    def __init__(self):
        self.delay_range = (1, 3)  # 随机延迟1-3秒

    def process_request(self, request, spider):
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)
```

### 数据管道处理

#### 验证管道
```python
class ValidationPipeline:
    def process_item(self, item, spider):
        """数据验证和清洗"""
        # 必填字段检查
        required_fields = ['title', 'price', 'supplier_id']
        for field in required_fields:
            if not item.get(field):
                raise DropItem(f"Missing required field: {field}")

        # 数据类型验证
        if item.get('price_min') and not isinstance(item['price_min'], (int, float)):
            item['price_min'] = float(item['price_min'])

        return item
```

#### 存储管道
```python
class StoragePipeline:
    def __init__(self, storage_service):
        self.storage = storage_service

    async def process_item(self, item, spider):
        """存储到数据库"""
        try:
            await self.storage.save_product(item)
            spider.crawler.stats.inc_value('success_count')
        except Exception as e:
            spider.logger.error(f"Failed to save item: {e}")
            spider.crawler.stats.inc_value('error_count')
            raise DropItem(f"Storage failed: {e}")

        return item
```

### 配置管理

#### 爬虫配置文件
```python
# settings.py
BOT_NAME = '1688sync'

# 基础配置
CONCURRENT_REQUESTS = 16
CONCURRENT_REQUESTS_PER_DOMAIN = 8
DOWNLOAD_DELAY = 1
RANDOMIZE_DOWNLOAD_DELAY = True
ROBOTSTXT_OBEY = True

# 中间件配置
DOWNLOADER_MIDDLEWARES = {
    'core.middlewares.UserAgentMiddleware': 400,
    'core.middlewares.ProxyMiddleware': 450,
    'core.middlewares.ThrottleMiddleware': 500,
}

# 管道配置
ITEM_PIPELINES = {
    'core.pipelines.ValidationPipeline': 300,
    'core.pipelines.StoragePipeline': 800,
}

# 扩展配置
EXTENSIONS = {
    'core.extensions.DatabaseExtension': 500,
    'core.extensions.StatsExtension': 500,
}
```

### 性能监控

#### 实时统计
```python
class SpiderStatsExtension:
    def __init__(self, stats, crawler):
        self.stats = stats
        self.crawler = crawler

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.stats, crawler)

    def spider_opened(self, spider):
        spider.logger.info(f"Spider {spider.name} opened")

    def spider_closed(self, spider):
        spider.logger.info(f"Spider {spider.name} closed")
        stats = self.stats.get_stats()
        spider.logger.info(f"Stats: {stats}")
```

## Effort Estimate

### 工作量评估
- **开发时间**: 20-25个工作日
- **任务分解**:
  - Scrapy项目架构搭建: 3天
  - 核心Spider开发: 8天
  - 反爬虫策略实现: 4天
  - 数据管道和中间件: 4天
  - 配置和监控体系: 3天
  - 测试和优化: 3-4天

### 技能要求
- **必备技能**:
  - Scrapy框架深度使用经验
  - XPath/CSS选择器熟练使用
  - HTTP协议和网络请求处理
  - Python异步编程

- **推荐技能**:
  - 代理IP池管理经验
  - 反爬虫技术研究
  - 数据清洗和处理
  - 性能调优经验

### 风险评估
- **高风险**: 1688反爬虫机制升级导致抓取失败
- **中风险**: 大并发下的稳定性问题
- **低风险**: 数据提取精度和完整性问题

## Dependencies

### 内部依赖
- **任务001**: 存储系统 - 提供数据存储接口

### 外部依赖
- Scrapy框架
- 代理IP服务提供商
- Redis（可选，用于去重）

## Quality Gates

- [ ] **功能测试**: 成功抓取1000个商品数据，无关键字段缺失
- [ ] **性能测试**: 平均抓取速度≥2商品/秒，成功率≥95%
- [ ] **稳定性测试**: 连续运行72小时无崩溃
- [ ] **代码质量**: 代码覆盖率≥80%，通过Code Review

## Deliverables

1. **完整的Scrapy爬虫项目代码**
2. **Spider配置文件和使用文档**
3. **反爬虫策略配置和说明**
4. **性能测试报告**
5. **API接口文档和使用示例**
